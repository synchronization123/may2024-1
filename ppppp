When dealing with a large CSV file (such as your 2GB file) in Python using pandas, you might encounter memory-related issues because pandas attempts to load the entire file into memory by default. To handle large files more efficiently and avoid memory errors, you can use the `chunksize` parameter in `pd.read_csv()` to read the file in chunks. Additionally, you can set `low_memory=True` to handle columns with mixed types more efficiently. Hereâ€™s how you can modify your script:

```python
import pandas as pd
import os

# Function to split a large CSV file into equal parts using pandas with chunking
def split_csv_pandas(input_file, output_prefix, num_parts):
    # Define chunk size based on estimated number of rows to fit comfortably in memory
    chunk_size = 10**6  # Adjust as needed based on your system's memory capacity

    # Reading CSV file in chunks
    chunks = pd.read_csv(input_file, chunksize=chunk_size, low_memory=True)
    
    part_number = 1
    for chunk in chunks:
        # Calculate rows per part
        rows_per_part = len(chunk) // num_parts
        
        for i in range(num_parts):
            start_index = i * rows_per_part
            end_index = (i + 1) * rows_per_part if i < num_parts - 1 else None
            part_df = chunk.iloc[start_index:end_index]
            
            # Write each part to a new CSV file
            part_filename = f"{output_prefix}_part{part_number}.csv"
            part_df.to_csv(part_filename, index=False)
            part_number += 1
    
    print(f"File split into {num_parts} parts successfully.")

# Example usage:
input_file = 'your_large_file.csv'  # Replace with your file path
output_prefix = 'output_split'      # Prefix for output files
num_parts = 6                       # Number of parts to split into

split_csv_pandas(input_file, output_prefix, num_parts)
```

### Explanation:

- **chunksize**: This parameter in `pd.read_csv()` specifies the number of rows to read into memory at once. Adjust `chunk_size` based on your system's memory capacity to balance between memory usage and performance.
  
- **low_memory=True**: This option helps pandas handle columns with mixed types more efficiently. It is useful when dealing with large files where pandas needs to infer the data types for each column.

- **Processing Chunks**: The script reads the CSV file in chunks (`chunksize=chunk_size`), then processes each chunk to split it into `num_parts` parts. This approach minimizes memory usage by only holding a chunk of data in memory at a time.

### Notes:

- **Memory Considerations**: Depending on your system's memory capacity, adjust `chunk_size` accordingly. Start with a smaller size and increase it gradually if your system can handle larger chunks without memory errors.

- **Performance**: Reading and writing large CSV files can be time-consuming. Adjust `chunk_size` and `num_parts` based on your specific requirements and system resources to achieve optimal performance.

By using chunking and setting `low_memory=True`, you can effectively handle large CSV files in pandas without encountering memory-related issues, thus enabling efficient processing and splitting of your 2GB CSV file into multiple parts.